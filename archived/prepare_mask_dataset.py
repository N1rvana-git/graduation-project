#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‡†å¤‡YOLOv7æ ¼å¼çš„å£ç½©æ£€æµ‹æ•°æ®é›†
"""

import os
import shutil
import random
from pathlib import Path
import json

def create_directory_structure():
    """åˆ›å»ºYOLOv7æ•°æ®é›†ç›®å½•ç»“æ„"""
    base_dir = Path("data/yolo_dataset")
    
    # åˆ›å»ºç›®å½•ç»“æ„
    dirs = [
        "images/train",
        "images/val", 
        "images/test",
        "labels/train",
        "labels/val",
        "labels/test"
    ]
    
    for dir_path in dirs:
        (base_dir / dir_path).mkdir(parents=True, exist_ok=True)
        
    print("âœ“ åˆ›å»ºYOLOv7æ•°æ®é›†ç›®å½•ç»“æ„")
    return base_dir

def convert_annotations_to_yolo(source_dir, target_dir, class_mapping):
    """å°†æ ‡æ³¨è½¬æ¢ä¸ºYOLOæ ¼å¼"""
    print(f"è½¬æ¢æ ‡æ³¨æ ¼å¼: {source_dir} -> {target_dir}")
    
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„æ ‡æ³¨æ ¼å¼è¿›è¡Œè½¬æ¢
    # ç¤ºä¾‹ï¼šå‡è®¾æœ‰JSONæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
    
    converted_count = 0
    
    # éå†æºç›®å½•ä¸­çš„æ ‡æ³¨æ–‡ä»¶
    for annotation_file in Path(source_dir).glob("*.json"):
        try:
            with open(annotation_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # è½¬æ¢ä¸ºYOLOæ ¼å¼
            yolo_annotations = []
            
            # å‡è®¾JSONæ ¼å¼åŒ…å«è¾¹ç•Œæ¡†ä¿¡æ¯
            if 'annotations' in data:
                img_width = data.get('image_width', 640)
                img_height = data.get('image_height', 640)
                
                for ann in data['annotations']:
                    class_name = ann.get('class', 'unknown')
                    if class_name in class_mapping:
                        class_id = class_mapping[class_name]
                        
                        # è·å–è¾¹ç•Œæ¡† (å‡è®¾æ ¼å¼ä¸º [x, y, width, height])
                        bbox = ann.get('bbox', [0, 0, 0, 0])
                        x, y, w, h = bbox
                        
                        # è½¬æ¢ä¸ºYOLOæ ¼å¼ (å½’ä¸€åŒ–çš„ä¸­å¿ƒç‚¹åæ ‡å’Œå®½é«˜)
                        x_center = (x + w/2) / img_width
                        y_center = (y + h/2) / img_height
                        norm_width = w / img_width
                        norm_height = h / img_height
                        
                        yolo_annotations.append(f"{class_id} {x_center:.6f} {y_center:.6f} {norm_width:.6f} {norm_height:.6f}")
            
            # ä¿å­˜YOLOæ ¼å¼æ ‡æ³¨
            output_file = target_dir / f"{annotation_file.stem}.txt"
            with open(output_file, 'w') as f:
                f.write('\n'.join(yolo_annotations))
                
            converted_count += 1
            
        except Exception as e:
            print(f"è½¬æ¢å¤±è´¥ {annotation_file}: {e}")
    
    print(f"âœ“ è½¬æ¢å®Œæˆ: {converted_count} ä¸ªæ ‡æ³¨æ–‡ä»¶")
    return converted_count

def create_sample_dataset():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    print("åˆ›å»ºç¤ºä¾‹å£ç½©æ£€æµ‹æ•°æ®é›†...")
    
    base_dir = create_directory_structure()
    
    # åˆ›å»ºç¤ºä¾‹æ ‡æ³¨æ–‡ä»¶
    sample_annotations = [
        # è®­ç»ƒé›†ç¤ºä¾‹
        ("train", "sample_001.txt", "1 0.5 0.3 0.2 0.4"),  # æˆ´å£ç½©
        ("train", "sample_002.txt", "0 0.4 0.5 0.3 0.3"),  # æœªæˆ´å£ç½©
        ("train", "sample_003.txt", "1 0.6 0.4 0.25 0.35"), # æˆ´å£ç½©
        
        # éªŒè¯é›†ç¤ºä¾‹
        ("val", "sample_004.txt", "0 0.45 0.4 0.28 0.32"),  # æœªæˆ´å£ç½©
        ("val", "sample_005.txt", "1 0.55 0.35 0.22 0.38"), # æˆ´å£ç½©
    ]
    
    for split, filename, annotation in sample_annotations:
        label_file = base_dir / "labels" / split / filename
        with open(label_file, 'w') as f:
            f.write(annotation)
    
    # åˆ›å»ºå¯¹åº”çš„å›¾åƒå ä½ç¬¦æ–‡ä»¶
    for split, filename, _ in sample_annotations:
        img_filename = filename.replace('.txt', '.jpg')
        img_file = base_dir / "images" / split / img_filename
        
        # åˆ›å»ºç©ºçš„å›¾åƒæ–‡ä»¶ä½œä¸ºå ä½ç¬¦
        img_file.touch()
    
    print("âœ“ åˆ›å»ºç¤ºä¾‹æ•°æ®é›†å®Œæˆ")
    
    # åˆ›å»ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    stats = {
        "total_images": len(sample_annotations),
        "train_images": len([x for x in sample_annotations if x[0] == "train"]),
        "val_images": len([x for x in sample_annotations if x[0] == "val"]),
        "classes": {
            "0": "no_mask",
            "1": "mask"
        }
    }
    
    with open(base_dir / "dataset_stats.json", 'w') as f:
        json.dump(stats, f, indent=2)
    
    return base_dir

def validate_dataset(dataset_dir):
    """éªŒè¯æ•°æ®é›†å®Œæ•´æ€§"""
    print("éªŒè¯æ•°æ®é›†å®Œæ•´æ€§...")
    
    dataset_dir = Path(dataset_dir)
    issues = []
    
    # æ£€æŸ¥ç›®å½•ç»“æ„
    required_dirs = [
        "images/train", "images/val",
        "labels/train", "labels/val"
    ]
    
    for dir_path in required_dirs:
        if not (dataset_dir / dir_path).exists():
            issues.append(f"ç¼ºå°‘ç›®å½•: {dir_path}")
    
    # æ£€æŸ¥å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶åŒ¹é…
    for split in ["train", "val"]:
        img_dir = dataset_dir / "images" / split
        label_dir = dataset_dir / "labels" / split
        
        if img_dir.exists() and label_dir.exists():
            img_files = set(f.stem for f in img_dir.glob("*"))
            label_files = set(f.stem for f in label_dir.glob("*.txt"))
            
            # æ£€æŸ¥å­¤ç«‹æ–‡ä»¶
            orphan_images = img_files - label_files
            orphan_labels = label_files - img_files
            
            if orphan_images:
                issues.append(f"{split}é›†ä¸­æœ‰ {len(orphan_images)} ä¸ªå›¾åƒç¼ºå°‘æ ‡æ³¨")
            if orphan_labels:
                issues.append(f"{split}é›†ä¸­æœ‰ {len(orphan_labels)} ä¸ªæ ‡æ³¨ç¼ºå°‘å›¾åƒ")
    
    # è¾“å‡ºéªŒè¯ç»“æœ
    if issues:
        print("âŒ æ•°æ®é›†éªŒè¯å‘ç°é—®é¢˜:")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print("âœ… æ•°æ®é›†éªŒè¯é€šè¿‡")
        return True

def main():
    """ä¸»å‡½æ•°"""
    print("=== YOLOv7å£ç½©æ£€æµ‹æ•°æ®é›†å‡†å¤‡å·¥å…· ===")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®é›†
    dataset_dir = Path("data/yolo_dataset")
    
    if dataset_dir.exists():
        print(f"æ•°æ®é›†ç›®å½•å·²å­˜åœ¨: {dataset_dir}")
        
        # éªŒè¯ç°æœ‰æ•°æ®é›†
        if validate_dataset(dataset_dir):
            print("âœ… ç°æœ‰æ•°æ®é›†éªŒè¯é€šè¿‡")
        else:
            print("âš ï¸ ç°æœ‰æ•°æ®é›†å­˜åœ¨é—®é¢˜")
    else:
        print("åˆ›å»ºæ–°çš„æ•°æ®é›†...")
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®é›†
        dataset_dir = create_sample_dataset()
        
        # éªŒè¯åˆ›å»ºçš„æ•°æ®é›†
        validate_dataset(dataset_dir)
    
    # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    print(f"\n=== æ•°æ®é›†ä¿¡æ¯ ===")
    print(f"æ•°æ®é›†è·¯å¾„: {dataset_dir.absolute()}")
    
    # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
    for split in ["train", "val", "test"]:
        img_dir = dataset_dir / "images" / split
        label_dir = dataset_dir / "labels" / split
        
        if img_dir.exists():
            img_count = len(list(img_dir.glob("*")))
            label_count = len(list(label_dir.glob("*.txt"))) if label_dir.exists() else 0
            print(f"{split}é›†: {img_count} å›¾åƒ, {label_count} æ ‡æ³¨")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_file = Path("yolov7/data/mask_detection.yaml")
    if config_file.exists():
        print(f"âœ… é…ç½®æ–‡ä»¶: {config_file}")
    else:
        print(f"âŒ é…ç½®æ–‡ä»¶ç¼ºå¤±: {config_file}")
    
    print("\nğŸ‰ æ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. å°†çœŸå®çš„å›¾åƒå’Œæ ‡æ³¨æ–‡ä»¶æ”¾å…¥å¯¹åº”ç›®å½•")
    print("2. è¿è¡Œè®­ç»ƒè„šæœ¬å¼€å§‹è®­ç»ƒ")
    print("3. ä½¿ç”¨ py models/train_yolov7_mask_detection.py å¼€å§‹è®­ç»ƒ")

if __name__ == "__main__":
    main()