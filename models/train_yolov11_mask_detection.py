
"""
YOLOv11å£ç½©æ£€æµ‹è®­ç»ƒè„šæœ¬
åŸºäºUltralyticså®˜æ–¹åº“å®ç°çš„YOLOv11næ¨¡å‹è®­ç»ƒ
ç¬¦åˆå¼€é¢˜æŠ¥å‘Šè¦æ±‚çš„æŠ€æœ¯è·¯çº¿
"""

# ä¿è¯modelsç­‰è‡ªå®šä¹‰åŒ…å¯è¢«import
import sys
import os
sys.path.append(os.path.abspath(os.path.dirname(__file__) + '/..'))

import os
import sys
import yaml
import torch
import argparse
import importlib.util
from pathlib import Path

try:
    from ultralytics import YOLO
    from ultralytics import settings
    # åŠ¨æ€æ³¨å†Œè‡ªå®šä¹‰æ¨¡å—ï¼Œç¡®ä¿YAMLè§£æå™¨èƒ½æ‰¾åˆ° GAMAttention
    from models.modules.attention import GAMAttention
    import ultralytics.nn.modules.block
    import ultralytics.nn.tasks
    
    # çŒ´å­è¡¥ä¸ï¼šå°† GAMAttention æ³¨å…¥åˆ° ultralytics çš„æ¨¡å—æŸ¥æ‰¾è·¯å¾„ä¸­
    setattr(ultralytics.nn.modules.block, 'GAMAttention', GAMAttention)
    setattr(ultralytics.nn.tasks, 'GAMAttention', GAMAttention)
except ImportError:
    print("é”™è¯¯ï¼šæœªå®‰è£…ultralyticsåº“")
    print("è¯·è¿è¡Œï¼špip install ultralytics")
    sys.exit(1)


class YOLOv11MaskDetectionTrainer:
    """YOLOv11å£ç½©æ£€æµ‹è®­ç»ƒå™¨"""
    
    def __init__(self,
                 data_path="data/mask_detection.yaml",
                 model_size="yolo11n",
                 img_size=640,
                 batch_size=16,
                 epochs=100,
                 device="0",
                 project="runs/yolov11_mask_detection",
                 workers: int | None = None,
                 export_onnx: bool = True):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            data_path: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
            model_size: æ¨¡å‹å¤§å° (yolo11n, yolo11s, yolo11mç­‰)
            img_size: è¾“å…¥å›¾åƒå°ºå¯¸
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆ-1è¡¨ç¤ºè‡ªåŠ¨ä¼˜åŒ–ï¼‰
            epochs: è®­ç»ƒè½®æ•°
            device: è®¾å¤‡ (cpu, 0, 1ç­‰)
            project: è®­ç»ƒè¾“å‡ºç›®å½•
        """
        self.data_path = data_path
        self.model_size = model_size
        self.img_size = img_size
        self.batch_size = batch_size
        self.epochs = epochs
        self.device = device
        self.project = project
        
        # è‡ªåŠ¨é…ç½® workers
        if workers is not None:
            self.workers = workers
        elif os.name == 'nt':
            # === Prof. Edge ä¿®å¤ ===
            # Windows æ˜¾å­˜/å†…å­˜ç´§ç¼ºæ—¶çš„ç»ˆæä¿åº•æ–¹æ¡ˆï¼šä½¿ç”¨å•çº¿ç¨‹ (0)
            # åŸå§‹è®¾ç½®æ˜¯ 2ï¼Œä½†ä½ çš„ç¯å¢ƒä¾ç„¶æŠ¥é”™ï¼Œè¯´æ˜èµ„æºéå¸¸ç´§å¼ ï¼Œå¿…é¡»é™ä¸º 0
            self.workers = 0 
            print(f"âš ï¸ Windowsç³»ç»Ÿæ£€æµ‹åˆ°å†…å­˜å‹åŠ›ï¼Œå¼ºåˆ¶ DataLoader workers={self.workers} (å•çº¿ç¨‹æ¨¡å¼)")
        else:
            self.workers = max(0, min((os.cpu_count() or 1) - 1, 8))

        self.export_onnx_flag = export_onnx
        
        # è®¾ç½®è·¯å¾„
        self.project_root = Path(__file__).parent.parent
        self.weights_dir = self.project_root / "weights"
        self.runs_dir = self.project_root / "runs"
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.weights_dir.mkdir(exist_ok=True)
        self.runs_dir.mkdir(exist_ok=True)
        
        # GPUä¼˜åŒ–
        if self.batch_size == -1:
            self.batch_size = self._auto_optimize_batch_size()
    
    def _auto_optimize_batch_size(self):
        """æ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨ä¼˜åŒ–æ‰¹æ¬¡å¤§å°"""
        if not torch.cuda.is_available():
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒï¼ˆbatch_size=8ï¼‰")
            return 8
        
        # è·å–GPUä¿¡æ¯
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ” æ£€æµ‹åˆ°GPUæ˜¾å­˜: {gpu_memory:.1f}GB")
        
        # RTX 3050 Laptop GPU = 4GBæ˜¾å­˜
        # æ ¹æ®å¼€é¢˜æŠ¥å‘Šçš„ç¡¬ä»¶çº¦æŸä¼˜åŒ–
        if gpu_memory <= 4:
            # YOLOv11nåœ¨4GBæ˜¾å­˜ä¸‹çš„æ¨èé…ç½®
            batch_size = 32
            print(f"âœ… è‡ªåŠ¨é…ç½®æ‰¹æ¬¡å¤§å°: {batch_size} (é€‚é…4GBæ˜¾å­˜)")
        elif gpu_memory <= 8:
            batch_size = 64
            print(f"âœ… è‡ªåŠ¨é…ç½®æ‰¹æ¬¡å¤§å°: {batch_size} (é€‚é…8GBæ˜¾å­˜)")
        else:
            batch_size = 128
            print(f"âœ… è‡ªåŠ¨é…ç½®æ‰¹æ¬¡å¤§å°: {batch_size} (å……è¶³æ˜¾å­˜)")
        
        return batch_size
    
    def verify_data_config(self):
        """éªŒè¯æ•°æ®é›†é…ç½®"""
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"æ•°æ®é›†é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['train', 'val', 'nc', 'names']
        for field in required_fields:
            if field not in config:
                raise ValueError(f"æ•°æ®é›†é…ç½®ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
        
        # éªŒè¯ç±»åˆ«æ•°ä¸åç§°ä¸€è‡´æ€§
        names = config.get('names', [])
        if not isinstance(names, list) or not names:
            raise ValueError("æ•°æ®é›†é…ç½®ä¸­çš„ names å¿…é¡»æ˜¯éç©ºåˆ—è¡¨")
        if config['nc'] != len(names):
            raise ValueError(f"ç±»åˆ«æ•°ä¸åç§°æ•°é‡ä¸ä¸€è‡´ï¼šnc={config['nc']}ï¼Œnamesæ•°={len(names)}")
        
        dataset_root = Path(config.get('path', '.')).expanduser()
        if not dataset_root.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ ¹ç›®å½•ä¸å­˜åœ¨: {dataset_root}")

        for split in ('train', 'val', 'test'):
            split_dir = config.get(split)
            if not split_dir:
                raise ValueError(f"æ•°æ®é›†é…ç½®ç¼ºå°‘ {split} è·¯å¾„")

            split_path = Path(split_dir)
            if not split_path.is_absolute():
                split_path = dataset_root / split_dir

            if not split_path.exists():
                raise FileNotFoundError(f"{split} è·¯å¾„ä¸å­˜åœ¨: {split_path}")

        print(f"âœ… æ•°æ®é›†ç±»åˆ«é…ç½®: {config['nc']} ç±» -> {names}")
        
        print(f"âœ… æ•°æ®é›†é…ç½®éªŒè¯é€šè¿‡")
        print(f"   - è®­ç»ƒé›†: {config['train']}")
        print(f"   - éªŒè¯é›†: {config['val']}")
        print(f"   - ç±»åˆ«æ•°: {config['nc']}")
        print(f"   - ç±»åˆ«å: {config['names']}")
        
        return config
    
    def train(self):
        """å¼€å§‹è®­ç»ƒï¼ˆæ”¯æŒè‡ªå®šä¹‰ç»“æ„é­”æ”¹ï¼‰"""
        print("="*60)
        print("ğŸš€ YOLOv11n å£ç½©æ£€æµ‹è®­ç»ƒå¼€å§‹")
        print("="*60)
        print(f"æ¨¡å‹: {self.model_size}")
        print(f"å›¾åƒå°ºå¯¸: {self.img_size}")
        print(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"è®­ç»ƒè½®æ•°: {self.epochs}")
        print(f"è®¾å¤‡: {self.device}")
        print("="*60)

        # éªŒè¯æ•°æ®é›†
        self.verify_data_config()

        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰ç»“æ„
        use_custom = self.model_size in ["yolo11n_mask_custom", "custom"]
        if use_custom:
            custom_cfg_path = str(self.project_root / "models" / "configs" / "yolo11n_mask_custom.yaml")
            print(f"\nğŸ“¥ åŠ è½½è‡ªå®šä¹‰ç»“æ„: {custom_cfg_path}")
            print("   ğŸ‘‰ ç»“æ„åŒ…å«: GAMAttention + WIoU + P2Detect")
            
            # 1. å…ˆæ„å»ºè‡ªå®šä¹‰çš„ç½‘ç»œç»“æ„ (éšæœºåˆå§‹åŒ–)
            model = YOLO(custom_cfg_path)
            
            # 2. å…³é”®æ­¥éª¤ï¼šå°è¯•åŠ è½½ yolo11n.pt çš„é¢„è®­ç»ƒæƒé‡
            # è¿™å« "Partial Transfer Learning" (éƒ¨åˆ†è¿ç§»å­¦ä¹ )
            try:
                print("âš–ï¸  æ­£åœ¨å°è¯•è¿ç§»åŠ è½½ COCO é¢„è®­ç»ƒæƒé‡ (yolo11n.pt)...")
                # load() ä¼šè‡ªåŠ¨åŒ¹é…åå­—å’Œå½¢çŠ¶ç›¸åŒçš„å±‚ï¼Œè·³è¿‡ä¸åŒ¹é…çš„å±‚(å¦‚GAMéƒ¨åˆ†)
                model.load("yolo11n.pt") 
                print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸï¼(ä¸åŒ¹é…çš„å±‚å°†ä¿æŒéšæœºåˆå§‹åŒ–)")
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Š: æƒé‡è¿ç§»åŠ è½½é‡åˆ°é—®é¢˜: {e}")
                print("   (å¦‚æœæ˜¯å½¢çŠ¶ä¸åŒ¹é…å¼•èµ·çš„æŠ¥é”™ï¼Œé€šå¸¸ä¼šè‡ªåŠ¨è·³è¿‡ï¼Œä¸å½±å“è®­ç»ƒ)")
        else:
            print("\nğŸ“¥ åŠ è½½å®˜æ–¹åŸºå‡†æ¨¡å‹: yolo11n.pt")
            model = YOLO('yolo11n.pt')

        # è®­ç»ƒé…ç½®
        train_args = {
            'data': self.data_path,
            'epochs': self.epochs,  
            'imgsz': self.img_size,
            'batch': 16,    # æ˜¾å­˜å—é™ï¼Œç‰©ç† Batch è®¾ä¸º 16 (æˆ– 32ï¼Œè§†æ˜¾å­˜è€Œå®š)

            # nbs (Nominal Batch Size) è®¾ä¸º 64ã€‚
            'nbs': 64,      

            'device': self.device,
            'project': self.project,
            'name': 'final_gam_wiou_p2', # æ–°ç‰ˆP2ç»“æ„
            'exist_ok': True,

            # è®­ç»ƒè€å¿ƒï¼šæ‰©å¢æ•°æ®åé¿å…è¿‡æ—©åœæ­¢
            'patience': 300,

            # Recallæå‡å…³é”®å‚æ•°
            'optimizer': 'auto',
            'workers': self.workers,
            'amp': True,    # å¿…é¡»å¼€å¯æ··åˆç²¾åº¦ä»¥èŠ‚çœæ˜¾å­˜

            # å†»ç»“Backboneå‰10è½®ï¼Œä¿æŠ¤é¢„è®­ç»ƒç‰¹å¾
            'freeze': 10,

            # é’ˆå¯¹å°ç›®æ ‡/äººè„¸çš„å¢å¼º
            'mosaic': 1.0,
            'mixup': 0.25,       # å¢å¼ºæ··åˆ
            'copy_paste': 0.3,   # å¤§å¹…æå‡Copy-Paste

            # å‡ ä½•å¢å¼º
            'degrees': 20.0,    # å¢åŠ æ—‹è½¬è§’åº¦
            'translate': 0.1,
            'scale': 0.5,
            'shear': 2.5,       # å¢åŠ å‰ªåˆ‡
            'perspective': 0.0005,
            'flipud': 0.0,
            'fliplr': 0.5,

            # å…‰ç…§å¢å¼º
            'hsv_h': 0.015,
            'hsv_s': 0.7,
            'hsv_v': 0.4,
            
            # è®­ç»ƒç­–ç•¥
            'close_mosaic': 30, # æœ€å 30 è½®å…³é—­ Mosaicï¼Œå¼ºåŒ–çœŸå®åˆ†å¸ƒ
            'save': True,
            'save_period': 10,
            'plots': True,
            'verbose': True,
        }

        # å¼€å§‹è®­ç»ƒ
        try:
            print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒï¼ˆ{self.epochs}è½®ï¼‰...")
            results = model.train(**train_args) if not use_custom else model.train(**train_args)

            print("\n" + "="*60)
            print("âœ… è®­ç»ƒå®Œæˆï¼")
            print("="*60)

            # æ˜¾ç¤ºè®­ç»ƒç»“æœ
            print(f"\nğŸ“Š è®­ç»ƒç»“æœæ‘˜è¦:")
            if hasattr(results, 'results_dict'):
                print(f"   - æœ€ä½³mAP@0.5: {results.results_dict.get('metrics/mAP50(B)', 'N/A')}")
                print(f"   - æœ€ä½³mAP@0.5:0.95: {results.results_dict.get('metrics/mAP50-95(B)', 'N/A')}")
                print(f"   - æ¨¡å‹ä¿å­˜è·¯å¾„: {results.save_dir}")
            self.visualize_training_curves(getattr(results, 'save_dir', self.project))

            # å¯¼å‡ºONNXï¼ˆç”¨äºå¾®ä¿¡å°ç¨‹åºéƒ¨ç½²ï¼‰
            if self.export_onnx_flag:
                self.export_onnx(model, getattr(results, 'save_dir', self.project))

            return results

        except Exception as e:
            print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
    
    def export_onnx(self, model, save_dir):
        """å¯¼å‡ºONNXæ ¼å¼ï¼ˆå¼€é¢˜æŠ¥å‘Šè¦æ±‚ï¼šâ‰¤80MBï¼‰"""
        print(f"\nğŸ“¦ å¯¼å‡ºONNXæ ¼å¼...")
        try:
            self._ensure_onnx_dependencies()

            onnx_path = model.export(
                format='onnx',
                imgsz=self.img_size,
                simplify=True,
                dynamic=False,
            )
            
            # æ£€æŸ¥æ¨¡å‹å¤§å°
            onnx_size = os.path.getsize(onnx_path) / 1024 / 1024  # MB
            print(f"âœ… ONNXæ¨¡å‹å·²å¯¼å‡º: {onnx_path}")
            print(f"   - æ¨¡å‹å¤§å°: {onnx_size:.2f}MB")
            
            if onnx_size > 80:
                print(f"âš ï¸ è­¦å‘Šï¼šæ¨¡å‹å¤§å°({onnx_size:.2f}MB)è¶…è¿‡å¼€é¢˜æŠ¥å‘Šè¦æ±‚(80MB)")
                print(f"   å»ºè®®ï¼šä½¿ç”¨é‡åŒ–æˆ–å‰ªæè¿›ä¸€æ­¥å‹ç¼©")
            else:
                print(f"âœ… æ¨¡å‹å¤§å°ç¬¦åˆå¼€é¢˜æŠ¥å‘Šè¦æ±‚(<80MB)")
            
        except Exception as e:
            print(f"âš ï¸ ONNXå¯¼å‡ºå¤±è´¥: {e}")
    
    def _ensure_onnx_dependencies(self):
        required = {
            'onnx': 'onnx==1.19.1',
            'onnxslim': 'onnxslim>=0.1.71',
            'onnxruntime': 'onnxruntime-gpu',
        }

        missing = []
        for module_name, pip_name in required.items():
            if importlib.util.find_spec(module_name) is None:
                missing.append(pip_name)

        if missing:
            install_cmd = 'pip install ' + ' '.join(missing)
            raise RuntimeError(
                "ç¼ºå°‘ONNXå¯¼å‡ºä¾èµ–: "
                + ', '.join(missing)
                + f"ã€‚è¯·å…ˆè¿è¡Œ: {install_cmd}"
            )

    def visualize_training_curves(self, save_dir):
        """æ ¹æ®results.csvç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        save_dir = Path(save_dir)
        csv_path = save_dir / 'results.csv'

        if not csv_path.exists():
            print(f"âš ï¸ æœªæ‰¾åˆ°results.csvï¼Œè·³è¿‡è®­ç»ƒæ›²çº¿ç»˜åˆ¶: {csv_path}")
            return

        try:
            import pandas as pd
            import matplotlib.pyplot as plt
        except ImportError as exc:
            print(f"âš ï¸ æ— æ³•ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼Œç¼ºå°‘ä¾èµ–: {exc}")
            return

        df = pd.read_csv(csv_path)
        if 'epoch' not in df.columns:
            print("âš ï¸ results.csv ç¼ºå°‘epochåˆ—ï¼Œè·³è¿‡è®­ç»ƒæ›²çº¿ç»˜åˆ¶")
            return

        metrics = [
            ("Box Loss", 'train/box_loss'),
            ("Cls Loss", 'train/cls_loss'),
            ("DFL Loss", 'train/dfl_loss'),
            ("mAP@0.5", 'metrics/mAP50(B)'),
            ("mAP@0.5:0.95", 'metrics/mAP50-95(B)'),
            ("Precision", 'metrics/precision(B)'),
            ("Recall", 'metrics/recall(B)'),
        ]

        plotted = [(title, col) for title, col in metrics if col in df.columns]
        if not plotted:
            print("âš ï¸ results.csv ä¸­æ²¡æœ‰å¯ç»˜åˆ¶çš„æ ‡å‡†åˆ—")
            return

        import math
        import numpy as np

        pr_df = None
        if {'metrics/precision(B)', 'metrics/recall(B)'}.issubset(df.columns):
            candidate = df[['metrics/recall(B)', 'metrics/precision(B)']].dropna()
            if not candidate.empty:
                pr_df = candidate.sort_values('metrics/recall(B)')

        total_plots = len(plotted) + (1 if pr_df is not None else 0)
        rows = math.ceil(total_plots / 2)
        fig, axes = plt.subplots(rows, 2, figsize=(12, 4 * rows))
        axes = axes.flatten()

        next_axis = 0
        for title, column in plotted:
            ax = axes[next_axis]
            ax.plot(df['epoch'], df[column], label=title, color='#1f77b4')
            ax.set_title(title)
            ax.set_xlabel('Epoch')
            ax.set_ylabel(column)
            ax.grid(True, linestyle='--', alpha=0.4)
            next_axis += 1

        if {'metrics/precision(B)', 'metrics/recall(B)'}.issubset(df.columns):
            # è®¡ç®— F1 Score
            # F1 = 2 * (P * R) / (P + R)
            p = df['metrics/precision(B)']
            r = df['metrics/recall(B)']
            f1 = 2 * (p * r) / (p + r + 1e-16)
            
            ax = axes[next_axis]
            ax.plot(df['epoch'], f1, color='#2ca02c', label='F1 Score')
            ax.set_title(f'F1 Score Curve (Max={f1.max():.3f})')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('F1 Score')
            ax.set_ylim(0, 1)
            ax.grid(True, linestyle='--', alpha=0.4)
            ax.legend(loc='lower right')
            next_axis += 1

        for idx in range(next_axis, len(axes)):
            axes[idx].axis('off')

        fig.tight_layout()
        output_path = save_dir / 'training_curves.png'
        fig.savefig(output_path, dpi=200)
        plt.close(fig)
        print(f"âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {output_path}")

    def validate(self, weights_path=None):
        """éªŒè¯æ¨¡å‹"""
        print(f"\nğŸ” å¼€å§‹éªŒè¯...")
        
        if weights_path:
            model = YOLO(weights_path)
        else:
            model = YOLO(f'{self.model_size}.pt')
        
        results = model.val(
            data=self.data_path,
            imgsz=self.img_size,
            batch=self.batch_size,
            device=self.device,
        )
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='YOLOv11å£ç½©æ£€æµ‹è®­ç»ƒ')
    parser.add_argument('--data', type=str, default='data/mask_detection.yaml', 
                       help='æ•°æ®é›†é…ç½®æ–‡ä»¶')
    parser.add_argument('--model', type=str, default='yolo11n', 
                       help='æ¨¡å‹å¤§å° (yolo11n, yolo11s, yolo11mç­‰)')
    parser.add_argument('--img-size', type=int, default=640, 
                       help='è¾“å…¥å›¾åƒå°ºå¯¸')
    parser.add_argument('--batch-size', type=int, default=-1, 
                       help='æ‰¹æ¬¡å¤§å°ï¼ˆ-1è¡¨ç¤ºè‡ªåŠ¨ä¼˜åŒ–ï¼‰')
    parser.add_argument('--epochs', type=int, default=100, 
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--device', type=str, default='0', 
                       help='è®¾å¤‡ (cpu, 0, 1ç­‰)')
    parser.add_argument('--project', type=str, default='runs/yolov11_mask_detection',
                       help='è®­ç»ƒè¾“å‡ºç›®å½•')
    parser.add_argument('--workers', type=int, default=None,
                       help='DataLoaderçº¿ç¨‹æ•°é‡ï¼ˆé»˜è®¤è‡ªåŠ¨ï¼‰')
    parser.add_argument('--no-export-onnx', action='store_true',
                       help='è®­ç»ƒç»“æŸåä¸å¯¼å‡ºONNXæ–‡ä»¶')
    parser.add_argument('--validate', action='store_true',
                       help='ä»…éªŒè¯æ¨¡å‹')
    parser.add_argument('--weights', type=str, default=None,
                       help='éªŒè¯æ—¶ä½¿ç”¨çš„æƒé‡æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = YOLOv11MaskDetectionTrainer(
        data_path=args.data,
        model_size=args.model,
        img_size=args.img_size,
        batch_size=args.batch_size,
        epochs=args.epochs,
        device=args.device,
    project=args.project,
    workers=args.workers,
    export_onnx=not args.no_export_onnx
    )
    
    # æ‰§è¡Œè®­ç»ƒæˆ–éªŒè¯
    if args.validate:
        trainer.validate(args.weights)
    else:
        trainer.train()


if __name__ == "__main__":
    main()
