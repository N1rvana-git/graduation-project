#!/usr/bin/env python3
"""
GPUç¯å¢ƒé…ç½®è„šæœ¬
æ£€æŸ¥å¹¶é…ç½®CUDAç¯å¢ƒï¼Œå®‰è£…æ”¯æŒGPUçš„PyTorch
"""

import subprocess
import sys
import os
import platform

def run_command(cmd, shell=True):
    """è¿è¡Œå‘½ä»¤å¹¶è¿”å›ç»“æœ"""
    try:
        result = subprocess.run(cmd, shell=shell, capture_output=True, text=True, encoding='utf-8')
        return result.returncode == 0, result.stdout, result.stderr
    except Exception as e:
        return False, "", str(e)

def check_python_env():
    """æ£€æŸ¥Pythonç¯å¢ƒ"""
    print("=== Pythonç¯å¢ƒæ£€æŸ¥ ===")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"Pythonè·¯å¾„: {sys.executable}")
    
    # æ£€æŸ¥pip
    success, stdout, stderr = run_command([sys.executable, "-m", "pip", "--version"], shell=False)
    if success:
        print(f"pipç‰ˆæœ¬: {stdout.strip()}")
    else:
        print("pipæœªæ­£ç¡®å®‰è£…")
        return False
    return True

def check_cuda_env():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("\n=== CUDAç¯å¢ƒæ£€æŸ¥ ===")
    
    # æ£€æŸ¥nvidia-smi
    success, stdout, stderr = run_command("nvidia-smi")
    if success:
        lines = stdout.split('\n')
        for line in lines:
            if 'CUDA Version:' in line:
                cuda_version = line.split('CUDA Version:')[1].strip().split()[0]
                print(f"CUDAé©±åŠ¨ç‰ˆæœ¬: {cuda_version}")
                return cuda_version
    else:
        print("æœªæ£€æµ‹åˆ°NVIDIA GPUæˆ–CUDAé©±åŠ¨")
        return None

def check_current_torch():
    """æ£€æŸ¥å½“å‰PyTorchå®‰è£…æƒ…å†µ"""
    print("\n=== PyTorchç¯å¢ƒæ£€æŸ¥ ===")
    try:
        import torch
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            for i in range(torch.cuda.device_count()):
                print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        return True
    except ImportError:
        print("PyTorchæœªå®‰è£…")
        return False

def install_pytorch_gpu():
    """å®‰è£…æ”¯æŒGPUçš„PyTorch"""
    print("\n=== å®‰è£…PyTorch GPUç‰ˆæœ¬ ===")
    
    # æ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©åˆé€‚çš„PyTorchç‰ˆæœ¬
    # CUDA 12.x ä½¿ç”¨ cu121
    install_cmd = [
        sys.executable, "-m", "pip", "install", 
        "torch", "torchvision", "torchaudio", 
        "--index-url", "https://download.pytorch.org/whl/cu121"
    ]
    
    print("æ­£åœ¨å®‰è£…PyTorch GPUç‰ˆæœ¬...")
    print(f"å‘½ä»¤: {' '.join(install_cmd)}")
    
    success, stdout, stderr = run_command(install_cmd, shell=False)
    if success:
        print("PyTorch GPUç‰ˆæœ¬å®‰è£…æˆåŠŸ!")
        return True
    else:
        print(f"å®‰è£…å¤±è´¥: {stderr}")
        return False

def install_other_dependencies():
    """å®‰è£…å…¶ä»–GPUç›¸å…³ä¾èµ–"""
    print("\n=== å®‰è£…å…¶ä»–ä¾èµ– ===")
    
    dependencies = [
        "ultralytics",
        "opencv-python",
        "numpy",
        "matplotlib",
        "Pillow",
        "PyYAML",
        "tqdm"
    ]
    
    for dep in dependencies:
        print(f"å®‰è£… {dep}...")
        success, stdout, stderr = run_command([sys.executable, "-m", "pip", "install", dep], shell=False)
        if success:
            print(f"âœ… {dep} å®‰è£…æˆåŠŸ")
        else:
            print(f"âŒ {dep} å®‰è£…å¤±è´¥: {stderr}")

def test_gpu_setup():
    """æµ‹è¯•GPUé…ç½®"""
    print("\n=== GPUé…ç½®æµ‹è¯• ===")
    
    test_code = '''
import torch
import time

print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
    
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"GPU {i} æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB")
    
    # ç®€å•çš„GPUè®¡ç®—æµ‹è¯•
    device = torch.device("cuda:0")
    print(f"\\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæµ‹è¯•å¼ é‡
    x = torch.randn(1000, 1000, device=device)
    y = torch.randn(1000, 1000, device=device)
    
    # è®¡æ—¶GPUè®¡ç®—
    start_time = time.time()
    z = torch.matmul(x, y)
    torch.cuda.synchronize()  # ç­‰å¾…GPUè®¡ç®—å®Œæˆ
    gpu_time = time.time() - start_time
    
    print(f"GPUçŸ©é˜µä¹˜æ³•è€—æ—¶: {gpu_time:.4f}ç§’")
    print("âœ… GPUé…ç½®æµ‹è¯•æˆåŠŸ!")
else:
    print("âŒ CUDAä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®‰è£…")
'''
    
    success, stdout, stderr = run_command([sys.executable, "-c", test_code], shell=False)
    if success:
        print(stdout)
    else:
        print(f"æµ‹è¯•å¤±è´¥: {stderr}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹é…ç½®GPUç¯å¢ƒ...")
    
    # 1. æ£€æŸ¥Pythonç¯å¢ƒ
    if not check_python_env():
        print("âŒ Pythonç¯å¢ƒæœ‰é—®é¢˜ï¼Œè¯·å…ˆè§£å†³Pythonå®‰è£…é—®é¢˜")
        return
    
    # 2. æ£€æŸ¥CUDAç¯å¢ƒ
    cuda_version = check_cuda_env()
    if not cuda_version:
        print("âŒ æœªæ£€æµ‹åˆ°CUDAç¯å¢ƒ")
        return
    
    # 3. æ£€æŸ¥å½“å‰PyTorch
    torch_installed = check_current_torch()
    
    # 4. å®‰è£…PyTorch GPUç‰ˆæœ¬
    if not torch_installed or input("æ˜¯å¦é‡æ–°å®‰è£…PyTorch GPUç‰ˆæœ¬? (y/n): ").lower() == 'y':
        if install_pytorch_gpu():
            print("âœ… PyTorch GPUç‰ˆæœ¬å®‰è£…å®Œæˆ")
        else:
            print("âŒ PyTorchå®‰è£…å¤±è´¥")
            return
    
    # 5. å®‰è£…å…¶ä»–ä¾èµ–
    install_other_dependencies()
    
    # 6. æµ‹è¯•GPUé…ç½®
    test_gpu_setup()
    
    print("\nğŸ‰ GPUç¯å¢ƒé…ç½®å®Œæˆ!")
    print("ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨GPUè¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæ¨ç†äº†ã€‚")

if __name__ == "__main__":
    main()